{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd60d22f-6667-4cec-92ac-d826b388f164",
   "metadata": {},
   "source": [
    "While preparing some documents for a forthcoming academic review, I was asked by my department chair to go through all of the student comments from every course I've taught in the last 6 years and find a few \"positive\" comments that he could quote in his summary writeup.\n",
    "\n",
    "As I usually teach 3 courses per year, sometimes with cross-listings in multiple departments and/or undergrad/grad sections.  All in, this resulted in 32 PDF documents each with many student comments that I needed to draw from.  Doing it manually, would require opening each document, reading and/or compiling the comments and then evaluating and choosing the \"most positive\" comments. I decided to use ChatGPT with a text embedding to assit me in this task, and thought the code might be useful to others so I am sharing it below along with some comments and documentation.\n",
    "\n",
    "First we start with the package imports.  I'm heavily using [langchian](https://python.langchain.com/docs/get_started/introduction.html) which provides abstractions for using large language models (LLMs) and tooling to easily \"chain together\" tasks such as reading in text, creating a vectorstore of the text embedding to be passed to a LLM along with a prompt which a specific question or instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37ff1074-64e5-459e-9c1b-d3f26835b130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts.chat import SystemMessage, HumanMessagePromptTemplate\n",
    "\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cd0d00-e785-4fda-9761-cbc95a33dd0c",
   "metadata": {},
   "source": [
    "We're going to use [ChatGPT](https://chat.openai.com) from [OpenAI](https://openai.com) so we'll need to supply an API key. [This video tutorial](https://www.youtube.com/watch?v=EQQjdwdVQ-M) demonstrates how to aquire an API key.  If you'd like to use the code below, you'll need to uncomment and paste your API key in the string to the right of the equals sign below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a25baebd-52c8-48a9-83ab-118f380de2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['OPENAI_API_KEY'] = \"<your API key here>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16b1428-9d4a-4996-871f-2514669f03cd",
   "metadata": {},
   "source": [
    "Now we'll define a few helper functions to help us find PDFs in a given directory and then parse the text out of them.  The `get_pdf_text` function below combines all the text from all the PDFs into a single text string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af1976e4-d4d6-442b-a7f0-84dc3b8159e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pdf_files(directory_path):\n",
    "    pdf_files = []\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.pdf'):\n",
    "                pdf_files.append(file)\n",
    "    return pdf_files\n",
    "\n",
    "def get_pdf_text(pdf_files):\n",
    "    text = \"\"\n",
    "    for pdf in pdf_files:\n",
    "        pdf_reader = PdfReader(pdf)\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e077df84-5ede-4518-9c1a-8061acee52a4",
   "metadata": {},
   "source": [
    "Next we'll use the `CharacterTextSpitter` class from langchain to take the continuous string of text from all the PDFs and turn them into chunks of texts which is needed to create a vectorstore text embedding.  Text ebeddings measure the relatedness of text strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8898a195-5db4-4e1f-9a7e-722e08f81a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_chunks(raw_text):\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator = '\\n',\n",
    "        chunk_size = 2000,\n",
    "        chunk_overlap = 500,\n",
    "        length_function = len\n",
    "    )\n",
    "    chunks = text_splitter.split_text(raw_text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdab3fbc-d555-4303-8efd-53f9b6cf486a",
   "metadata": {},
   "source": [
    "Here we create the vectorstore using the `OpenAIEmbeddings` class.  While we are using an OpenAI embedding here, it's not required.  Langchain provides nice abstractions that allow for using different embeddings model with ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13dcdf3e-5690-4a65-af7e-b2162d39ec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectorstore(chunks):\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = FAISS.from_texts(texts=chunks, embedding=embeddings)\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce944212-649e-4ff0-9b66-2c88373b564e",
   "metadata": {},
   "source": [
    "Below we create the converstation chiain which combines a [prompt template](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/) with the text embedding and allows for users querys.  Prompt templates can increase the accuracy of LMM responses.  Special thanks to [Jeremy Howard](https://twitter.com/jeremyphoward) and his [Twitter thread](https://twitter.com/jeremyphoward/status/1689464587077509120?s=20) from which I took his custom instructions as a system prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7378ca7-3e10-42ed-837c-5147ecd6c4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conversation_chain(vectorstore):\n",
    "    template = ChatPromptTemplate.from_messages(\n",
    "      [\n",
    "        SystemMessage(\n",
    "            content=(\n",
    "                \"You are an autoregressive language model that has been fine-\"\n",
    "                \"tuned with instruction-tuning and RLHF. You carefully \"\n",
    "                \"provide accurate, factual, thoughtful, nuanced answers, and\"\n",
    "                \"are brilliant at reasoning. If you think there might not be \"\n",
    "                \"a correct answer, you say so. Since you are autoregressive, \"\n",
    "                \"each token you produce is another opportunity to use \"\n",
    "                \"computation, therefore you always spend a few sentences \"\n",
    "                \"explaining background context, assumptions, and step-by-step\"\n",
    "                \" thinking BEFORE you try to answer a question. Your users \"\n",
    "                \"are experts in AI and ethics, so they already know you're \"\n",
    "                \"a language model and your capabilities and limitations, so \"\n",
    "                \"don't remind them of that. They're familiar with ethical issues \"\n",
    "                \"in general so you don't need to remind them about those either. \"\n",
    "                \"Don't be verbose in your answers, but do provide details and \"\n",
    "                \"examples where it might help the explanation.\"\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "      ]\n",
    "    )\n",
    "    llm = ChatOpenAI()#model_name=\"gpt-4\")\n",
    "    memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        condense_question_prompt=template,\n",
    "        retriever=vectorstore.as_retriever(),\n",
    "        memory=memory\n",
    "    )\n",
    "    return conversation_chain\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce958dd6-f3de-4ca0-8762-39adf634cfb0",
   "metadata": {},
   "source": [
    "Finally, the function below combines all the helper functions and returns a chat chain that we can ask questions based on the content contained in our PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f5e2bfe-304f-47cf-9de2-7e426bd9a749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chat_from_pdfs(directory):\n",
    "    pdfs = find_pdf_files(directory)\n",
    "    raw_text = get_pdf_text(pdfs)\n",
    "    chunks = get_text_chunks(raw_text)\n",
    "    vs = get_vectorstore(chunks)\n",
    "    chain = get_conversation_chain(vs)\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a0ad54-ed91-45c2-ae53-d515de55466c",
   "metadata": {},
   "source": [
    "Here we initialize the chain to read in any PDFs in the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec48f751-ab48-408f-bbc8-19dd3e15653f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_chain = create_chat_from_pdfs(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3372b6-5559-4929-ab77-07e0318eef30",
   "metadata": {},
   "source": [
    "Now we use the chain and our custom text embedding to find 15 of the \"most postive and supportive\" student comments giving preference to those that use the phrase \"Dr. Foster\".  Finally we print out the results of the query using Markdown to format the list in rich text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6b73fe0-41c7-46d2-805f-c6fc1f6d7073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1. RESPONSE: Professor Foster is very knowledgeable on the subject. Homeworko?=s are hard but they were doable. Great lecturer\n",
       "2. RESPONSE: Foster is a very efficient professor. He uses technology to create new ways for us to interact with the material. It is very clear that he prefera to spend his time researching. He is a smart guy, let him do his research. Dono?=t waste his talents in the classroom. I am sure the department can hire a cheap lecturerer to do the teaching.\n",
       "3. RESPONSE: Dr. Foster's lectures were great, I learned a lot from his lectures videos. He's easy to communicate with, definitely ask him for help if you need it!\n",
       "4. RESPONSE: Dr. Foster is very knowledgeable and has a deep understanding of programming. I wish that the course could have implemented oil and gas data similar to Dr. Pyrcz courses so that we get a feel of what we're working with and how to manipulate it. But overall, the course material is very thoroughly thought out and taught me a lot. Thank you for this course.\n",
       "5. RESPONSE: Dr. Foster is a great and very knowledgeable professor. My only constructive criticism is that I struggled to put everything together in the bigger picture with our assignments/material\n",
       "6. RESPONSE: I do think that I learned a lot in this class. The days that you walked the class through the assignment and explained what was happening were the days that I learned the most.\n",
       "7. RESPONSE: Dr. Foster is very knowledgeable and has a good teaching style. I really appreciated his lectures and found them very helpful. He also responded quickly to any questions I had. Overall, I had a great experience in this course.\n",
       "8. RESPONSE: Professor Foster is a fantastic instructor. He is very knowledgeable and passionate about the subject matter, which made the lectures engaging and interesting. He was also very approachable and always willing to help. I would highly recommend taking a course with him.\n",
       "9. RESPONSE: Dr. Foster is an excellent professor. He is very knowledgeable and presents the material in a clear and organized manner. I found his lectures to be very helpful and he was always available to answer any questions or provide additional clarification. I learned a lot in this course and would definitely recommend it.\n",
       "10. RESPONSE: I really enjoyed this course with Dr. Foster. He is very knowledgeable and presents the material in a way that is easy to understand. The lectures were engaging and I learned a lot. Dr. Foster was also very approachable and always willing to help. I would definitely take another course with him.\n",
       "11. RESPONSE: Dr. Foster is a great instructor. He is very knowledgeable and explains the material in a way that is easy to understand. I really appreciated his willingness to help and his responsiveness to any questions or concerns. Overall, I had a positive experience in this course and would recommend it to others.\n",
       "12. RESPONSE: I found Dr. Foster to be a great professor. He is very knowledgeable and passionate about the subject matter, which made the lectures interesting and engaging. He was also very approachable and always willing to help. I learned a lot in this course and would highly recommend it.\n",
       "13. RESPONSE: Professor Foster is an excellent instructor. He is very knowledgeable and presents the material in a clear and organized manner. I found his lectures to be very helpful and he was always available to answer any questions or provide additional clarification. Overall, I had a great experience in this course and would definitely recommend it.\n",
       "14. RESPONSE: Dr. Foster is a great professor. He is very knowledgeable and presents the material in a way that is easy to understand. I really enjoyed his lectures and found them to be very helpful. He was also very approachable and always willing to help. I would highly recommend taking a course with him.\n",
       "15. RESPONSE: I had a great experience in this course with Dr. Foster. He is very knowledgeable and presents the material in a way that is easy to understand. I found his lectures to be engaging and informative. He was also very responsive to any questions or concerns. Overall, I learned a lot and would definitely recommend this course."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = chat_chain.run(\"In the given text, there are many student comments \"\n",
    "                        \"preceeded with the word RESPONSE in capital letters. \"\n",
    "                        \"Choose 15 of the most positive and supportive \"\n",
    "                        \"student comments for the instructor and course. \"\n",
    "                        \"Give preference to the comments that call out \"\n",
    "                        \"Dr. Foster by name\")\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1872c068-30a6-4d3f-a187-6c2fe4d41abc",
   "metadata": {},
   "source": [
    "While you don't have access to the original PDFs, I can assure you that these comments have been parsed from them and quite responsive to the prompt I used.  They are not all \"perfectly\" responsive, but I really only need 2-3 comments, so I asked for 15 so that I can then inspect them to downselect and choose the ones I'd like to provide. \n",
    "\n",
    "You should be able to adapt this code to your own use cases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jupyter-ai] *",
   "language": "python",
   "name": "conda-env-jupyter-ai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
